{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratnaan23/ds_class/blob/main/eng_datascience_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3 (라티나 아스투티) "
      ],
      "metadata": {
        "id": "9bRZzfHSMY7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 파이토치의 자동미분: 모든 것을 역전파하라\n",
        "## PyTorch's autograd: Backpropagating all things\n",
        "\n",
        "Simple example of backpropagation: Compute the gradient of a composition of functions with respect to their innermost parameters (w and b) by propagating derivatives backward using the chain rule.\n",
        "\n",
        "Basic requirement: All functions can be differentiated analytically."
      ],
      "metadata": {
        "id": "1uRhP3-wslMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **기울기 자동 계산** Computing the gradient automatically\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "PyTorch tensors can remember where they come from, in terms of the operations and parent tensors that originated them. They can also provide the chain of derivatives of such operations with respect to their inputs.\n",
        "\n",
        "With PyTorch’s autograd, given a forward expression, PyTorch will automatically provide the gradient of that expression with respect to its input parameters.\n"
      ],
      "metadata": {
        "id": "9AAjXt0XAebq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **자동미분 적용하기** Applying autograd\n",
        "\n",
        "Let's rewrite our thermometer callibration code using autograd:"
      ],
      "metadata": {
        "id": "njKhCkqpuG-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[5]:\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)"
      ],
      "metadata": {
        "id": "4Ifo6GxWuKlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The requires_grad=True argument is telling PyTorch to track the entire family tree of tensors resulting from operations on params.\n",
        "\n",
        "#### **미분 속성 사용하기** Using the grad attribute\n",
        "\n",
        "In general, all PyTorch tensors have an attribute named grad, normally it's None. \n",
        "\n",
        "To use the grad attribute, start with a tensor with requires_grad set to True, then call the model and compute the loss, and then call backward on the loss tensor."
      ],
      "metadata": {
        "id": "egvrmVTwuSqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[7]:\n",
        "loss = loss_fn(model(t_u, *params), t_c)\n",
        "loss.backward()\n",
        "\n",
        "params.grad"
      ],
      "metadata": {
        "id": "t5W44lm5uXI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[7]:\n",
        "tensor([4517.2969,   82.6000])\n",
        "```\n",
        "\n",
        "The grad attribute of params contains the derivatives of the loss with respect to each element of params.\n",
        "\n",
        "\n",
        "When calculating `loss`, PyTorch creates the autograd graph with the operations as nodes.\n",
        "\n",
        "\n",
        "\n",
        "When we call `loss.backward()', PyTorch traverses this graph in the reverse direction to compute gradients as shown by the arrows in the bottom row of the figure.\n",
        "\n",
        "![Figure 5.10](https://drive.google.com/uc?export=view&id=1xlHFCoLoqeu3RVtrVlK-_hBBTnIqHvCl)\n",
        "\n",
        "#### **미분 함수 누적하기** Accumulating the grad functions\n",
        "\n",
        "\n",
        "Calling backward will lead derivatives to accumulate at leaf nodes.\n",
        "\n",
        "\n",
        "We need to zero the gradient explicitly at each iteration, using the in-place zero_ method:"
      ],
      "metadata": {
        "id": "2SuQAEVcuhkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[8]:\n",
        "if params.grad is not None:\n",
        "    params.grad.zero_()"
      ],
      "metadata": {
        "id": "RHMtCWEqukYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our autograd-enabled training code will look like this:"
      ],
      "metadata": {
        "id": "GkinfUJ-unkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[9]:\n",
        "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        if params.grad is not None:      # loss.backward() 호출 전 아무 위치에나 두면 된다\n",
        "            params.grad.zero_()\n",
        "        \n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "        loss.backward()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            params -= learning_rate * params.grad\n",
        "        \n",
        "        if epoch % 500 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "CbZSVmL_uq8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In[10]: \n",
        "training_loop(\n",
        "    n_epochs = 5000, \n",
        "    learning_rate = le-2,\n",
        "    params = torch. tensor( [1.0, 0.0], requires _grad=True),\n",
        "    t_u = t_un,      # 여기서도 정규화된 t_un을 사용\n",
        "    t_c = t_c)"
      ],
      "metadata": {
        "id": "rkiGulNuuvbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out [10]:\n",
        "Epoch 500, Loss 7.860116 \n",
        "Epoch 1000, Loss 3.828538 \n",
        "Epoch 1500, Loss 3.092191\n",
        "Epoch 2000, Loss 2.957697\n",
        "Epoch 2500, Loss 2.933134\n",
        "Epoch 3000, Loss 2.928648 \n",
        "Epoch 3500, Loss 2.927830\n",
        "Epoch 4000, Loss 2.927679 \n",
        "Epoch 4500, Loss 2.927652 \n",
        "Epoch 5000, Loss 2.927647\n",
        "```\n",
        "\n",
        "\n",
        "We got the same result as before, no need to calculate by hands anymore."
      ],
      "metadata": {
        "id": "U0mGDEmBhFcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **골라쓰는 옵티마이저**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The torch module has an optim submodule where we can find classes implementing different optimization algorithm."
      ],
      "metadata": {
        "id": "sfPpwQe4vMfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[5]:\n",
        "import torch.optim as optim\n",
        "\n",
        "dir(optim)"
      ],
      "metadata": {
        "id": "8sbQ4dxyvPCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[5]:\n",
        "['ASGD',\n",
        " 'Adadelta',\n",
        " 'Adagrad',\n",
        " 'Adam',\n",
        " 'Adamax',\n",
        " 'LBFGS',\n",
        " 'Optimizer',\n",
        " 'RMSprop',\n",
        " 'Rprop',\n",
        " 'SGD',\n",
        " 'SparseAdam',\n",
        "...\n",
        "]\n",
        "```\n",
        "\n",
        "Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically with requires_grad set to True) as the first input. \n",
        "\n",
        "![Figure 5.11](https://drive.google.com/uc?export=view&id=1d1PWcpufgDU_bqcOll0YqMb7mXTJfJ7g)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "(A) Conceptual representation of how an optimizer holds a reference to parameters.\n",
        "\n",
        "(B) After a loss is computed from inputs,\n",
        "\n",
        "(C) a call to .backward leads to .grad being populated on parameters.\n",
        "\n",
        "(D) At that point, the optimizer can access .grad. and compute the parameter updates.\n",
        "\n",
        "#### **경사 하강 옵티마이저 사용하기** Using a gradient descent optimizer\n",
        "\n",
        "Let's create params and instantiate a gradient descent optimizer:"
      ],
      "metadata": {
        "id": "rR7S6bYzvT7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[6]:\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.SGD([params], lr = learning_rate)"
      ],
      "metadata": {
        "id": "jOmpAQ8IvYRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD stands for stochastic gradient descent. The term stochastic comes from the fact that the gradient is typically obtained by averaging over a random subset of all input samples, called a minibatch."
      ],
      "metadata": {
        "id": "lGD8a-1nvbEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[7]:\n",
        "t_p = model(t_u, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "loss.backward()\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "params"
      ],
      "metadata": {
        "id": "jmCst7MfveF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[7]:\n",
        "tensor([9.5483e-01, -8.2600e-04], requires_grad=True)\n",
        "```\n",
        "\n",
        "The value of params is updated upon calling step, because the optimizer looks into params.grad and updates params, substracting learning_rate times grad from it.\n",
        "\n",
        "But the code is not ready yet since we haven't zero out the gradients. The loop-ready code with the extra zero_grad right before the call to backward:"
      ],
      "metadata": {
        "id": "j5IEPfFaviet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[8]:\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr = learning_rate)\n",
        "\n",
        "t_p = model(t_un, *params)\n",
        "loss = loss_fn(t_p, t_c)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "params"
      ],
      "metadata": {
        "id": "R0ddC1Vpvk5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[8]:\n",
        "tensor([1.7761, 0.1064], requires_grad=True)\n",
        "```\n",
        "\n",
        "훈련 루프도 여기에 맞춰 고쳐보자:\n",
        "\n",
        "Our updated training loop:"
      ],
      "metadata": {
        "id": "S74QpxKvvoV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[9]:\n",
        "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "    \n",
        "    return params"
      ],
      "metadata": {
        "id": "ZdaFtmDRvsUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In[10]:\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)      # training_loop에 있는 params가 동일해야 한다.\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = t_un,\n",
        "    t_c = t_c)"
      ],
      "metadata": {
        "id": "iQp8tqvpvtkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[10]:\n",
        "Epoch 500, Loss 7.860118\n",
        "Epoch 1000, Loss 3.828538\n",
        "Epoch 1500, Loss 3.092191\n",
        "Epoch 2000, Loss 2.957697\n",
        "Epoch 2500, Loss 2.933134\n",
        "Epoch 3000, Loss 2.928648\n",
        "Epoch 3500, Loss 2.927830\n",
        "Epoch 4000, Loss 2.927680\n",
        "Epoch 4500, Loss 2.927651\n",
        "Epoch 5000, Loss 2.927648\n",
        "\n",
        "tensor([  5.3671, -17.3012], requires_grad=True)\n",
        "```\n",
        "#### **다른 옵티마이저 테스트하기** Testing other optimizer\n",
        "\n",
        "To test more optimizers, we have to instantiate a different optimizer, let's say Adam instead of SGD.\n",
        "\n",
        "\n",
        "\n",
        "In Adam optimizer, the learning rate is set adaptively and it is a lot less sensitive to the scaling of the parameters. We can use the non-normalize input t_u and increase the learning rate to 1e-1."
      ],
      "metadata": {
        "id": "wXyFAYFuv3In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[11]:\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-1\n",
        "optimizer = optim.Adam([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 2000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    t_u = t_u,\n",
        "    t_c = t_c)"
      ],
      "metadata": {
        "id": "Lj8KGwQQv7V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Out[11]:\n",
        "Epoch 500, Loss 7.612903\n",
        "Epoch 1000, Loss 3.086700\n",
        "Epoch 1500, Loss 2.928578\n",
        "Epoch 2000, Loss 2.927646\n",
        "\n",
        "tensor([  0.5367, -17.3021], requires_grad=True)\n",
        "```"
      ],
      "metadata": {
        "id": "rZinjigh3s7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **훈련, 검증, 과적함** Training, Validation, and Overfitting\n",
        "\n",
        "---\n",
        "\n",
        "A highly adaptable model will tend to use its many parameters to make sure the loss is minimal at the data points, but we'll have no guarantee that the model behaves well away from or in between the data points.\n",
        "\n",
        "\n",
        "To avoid overfitting, we must take a few data points out of our dataset (the validation set) and only fit our model on the remaining data point (the training set), as shown below.\n",
        "\n",
        "![Figure 5.12](https://drive.google.com/uc?export=view&id=1OLFLenUe0a0qX_NDI06kbFXYBumYjlJo)\n",
        "\n",
        "\n",
        "We can evaluate the loss on the training set and on the validation set, and look at both result to decide if we've done a good job fitting our model.\n",
        "\n",
        "#### **훈련 손실 평가하기** Evaluating the training loss\n",
        "\n",
        "\n",
        "\bIf the training loss is not decreasing, chances are:\n",
        "*   the model is too simple for the data; or\n",
        "*   our data doesn't contain meaningful information that lets it explain the output.\n",
        "\n",
        "\n",
        "#### **검증셋으로 일반화하기** Generalizing to the validation set\n",
        "\n",
        "\n",
        "If the training loss and the validation loss diverge, we're overfitting.\n",
        "\n",
        "\n",
        "\n",
        "In our thermomether case, if we decided to fit the data with a more complicated function, for example piecewise polynomial, it could generate a model meandering its way through the data points, just like the figure below. \n",
        "\n",
        "![Figure 5.13](https://drive.google.com/uc?export=view&id=1OgexRkWqoEa7LXE9Z6Ri4LLvKhiRQn-m)\n",
        "\n",
        "\n",
        "\n",
        "The model tries to push the loss very close to zero, but the behaviour of the function away from the data points does not increase the loss, so we have nothing to keep the model in check for inputs away from the training data points.\n",
        "\n",
        "What's the solution for overfitting?:\n",
        "*   make sure we get enough data for the process\n",
        "*   make sure the model capable of fitting the training data is as regular as possible in between them (we can add penalization terms to the loss function, or add noise to the input sample)\n",
        "*   make our model simpler (a simpler model may not fit the training data perfectly but it will likely behave more regularly in between data points)\n",
        "\n",
        "#### \b**데이터셋 나누기** Splitting a dataset\n",
        "\n",
        "\n",
        "Let's go back to our example to see how we can split the data into a training set and a validation set. \n",
        "\n",
        "\n",
        "\n",
        "We'll do it by shuffling t_u and t_c the same way and then splitting the resulting shuffled tensors into two parts.\n",
        "\n",
        "\n",
        "\n",
        "Shuffling the elements of a tensor amounts to finding a permutation of its indices. The randperm function does exactly this."
      ],
      "metadata": {
        "id": "4OelkboHwHEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[12]:\n",
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "\n",
        "train_indices, val_indices     # 랜덤이기 때문에 실행했을 때는 값이 다를 수도 있음"
      ],
      "metadata": {
        "id": "DRfeWgfewMtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[12]:\n",
        "(tensor([9, 6, 5, 8, 4, 7, 0, 1, 3]), tensor([ 2, 10]))\n",
        "```\n",
        "\n",
        "\n",
        "We just got index tensor that we can use to build training and validation sets starting from the data tensors:"
      ],
      "metadata": {
        "id": "bTTUFVH7wQp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[13]:\n",
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]\n",
        "\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ],
      "metadata": {
        "id": "3lLvV-GfwS1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training loop doesn’t really change. We just want to additionally evaluate the validation loss at every epoch, to have a chance to recognize whether we’re overfitting:"
      ],
      "metadata": {
        "id": "C9J4ljlSwVtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[14]:\n",
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
        "              train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "        val_t_p = model(val_t_u, *params)\n",
        "        val_loss = loss_fn(val_t_p, val_t_c)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()    # 검증 데이터로는 학슴하면 안 되므로 val_loss.backward()가 없다\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch <= 3 or epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
        "                  f\" Validation loss {val_loss.item():.4f}\")\n",
        "    \n",
        "    return params"
      ],
      "metadata": {
        "id": "1VqTZo_SwYZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In[15]:\n",
        "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params], lr=learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 3000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    # SGD를 사용하므로 정규화된 입력을 사용함\n",
        "    train_t_u = train_t_un,\n",
        "    val_t_u = val_t_un,\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)"
      ],
      "metadata": {
        "id": "wCkkuQ2cwa22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Out[15]:\n",
        "Epoch 1, Training loss 66.5811, Validation loss 142.3890\n",
        "Epoch 2, Training loss 38.8626, Validation loss 64.0434\n",
        "Epoch 3, Training loss 33.3475, Validation loss 39.4590\n",
        "Epoch 500, Training loss 7.1454, Validation loss 9.1252\n",
        "Epoch 1000, Training loss 3.5940, Validation loss 5.3110\n",
        "Epoch 1500, Training loss 3.0942, Validation loss 4.1611\n",
        "Epoch 2000, Training loss 3.0238, Validation loss 3.7693\n",
        "Epoch 2500, Training loss 3.0139, Validation loss 3.6279\n",
        "Epoch 3000, Training loss 3.0125, Validation loss 3.5756\n",
        "\n",
        "tensor([  5.1964, -16.7512], requires_grad=True)\n",
        "```\n",
        "\n",
        "Our validation set is really small, so the validation loss will only be meaningful up to a point.\n",
        "\n",
        "\n",
        "*   We expect a model to perform better on the training set\n",
        "*   our main goal is to see both the training loss and the validation loss decreasing.\n",
        "\n",
        "![Figure 5.14](https://drive.google.com/uc?export=view&id=1NOdCPVz0Y8sNSbbtljhnj2Zwkc03V2UQ)\n",
        "\n",
        "In the figure above (solid line = training; dotted line = validation),\n",
        "\n",
        "where in case A, the model seems to not learning at all, and in case B, we can see that the training loss is decreasing but the validation loss is increasing, which shows overfitting.  Case C is ideal, where both training and validation loss are decreasing. While case D is showing a similar trend of training and validation loss, which is an acceptable scenario."
      ],
      "metadata": {
        "id": "c2xWChKm36Ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자동미분의 주의사항과 자동미분 끄기 Autograd nits and switching it off\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In our training loop, our model is evaluated twice, once on train_t_u and once on val_t_u and then backward is called. The first line in the training loop evaluates model on train_t_u to produce train_t_p. Then train_loss is evaluated from train_t_p. This creates a computation graph that links train_t_u to train_t_p to train_loss. \n",
        "\n",
        "\n",
        "When model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this case, a separate computation graph will be created that links val_t_u to val_t_p to val_loss.\n",
        "\n",
        "\n",
        "Separate tensors have been run through the same functions, model and loss_fn, generating separate computation graphs, as shown in figure below.\n",
        "\n",
        "\n",
        "![Figure 5.15](https://drive.google.com/uc?export=view&id=1pmDLKyqQbx1vA52-6ndT_5fi5CKbTxll)\n",
        "\n",
        "\n",
        "\n",
        "The only tensors these two graphs have in common are the parameters. When we call backward on train_loss, we run backward on the first graph. In other words, we accumulate the derivatives of train_loss with respect to the parameters based on the computation generated from train_t_u. \n",
        "\n",
        "\n",
        "Another point, since we're not even calling backward on val_loss, we could switch off autograd on this part.\n",
        "\n",
        "\n",
        "\n",
        "PyTorch allows us to switch off autograd when we don't need it, using the torch.no_grad context manager.\n",
        "\n",
        "\n",
        "We can make sure by checking the value of the requires_grad attribute on the val_loss tensor."
      ],
      "metadata": {
        "id": "-qHnnIL94zdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[16]:\n",
        "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
        "                  train_t_c, val_t_c):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_t_p = model(train_t_u, *params)\n",
        "        train_loss = loss_fn(train_t_p, train_t_c)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_t_p = model(val_t_u, *params)\n",
        "            val_loss = loss_fn(val_t_p, val_t_c)\n",
        "            assert val_loss.requires_grad == False\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "WoVY9LoNwmt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Using the related set_grad_enabled context, we can also condition the code to run with autograd enabled or disabled, according to a Boolean expression.\n",
        "\n",
        "We could define a calc_forward function that takes data as input and runs model and loss_fn with or without autograd according to a Boolean is_train argument."
      ],
      "metadata": {
        "id": "1pWd_ztGwk_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In[17]:\n",
        "def calc_forward(t_u, t_c, is_train):\n",
        "    with torch.set_grad_enabled(is_train):\n",
        "        t_p = model(t_u, *params)\n",
        "        loss = loss_fn(t_p, t_c)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "yGJACc4Xwpww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Conclusion\n",
        "\n",
        "*   A model can be optimized to fit the data.\n",
        "*   Linear models are the simplest reasonable model to use to fit data.\n",
        "*   Data is often split into separate sets of training samples and validation samples.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0-epLLh42on"
      }
    }
  ]
}